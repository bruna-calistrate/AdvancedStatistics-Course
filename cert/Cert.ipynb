{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carregar biblioteca e funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibiotecas padrão\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Visualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "# Escrever as fórmulas dos modelos\n",
    "import patsy as pt\n",
    "\n",
    "# Bibioteca estatística\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Fator de inflação da variância\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Bibliotecas do scikitlearn para calcular as métricas e fazer a regressão regularizada\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedKFold, cross_val_score\n",
    "from sklearn.linear_model import Ridge, RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_null(df, percentiles=False):\n",
    "  \"\"\"Função para descrição de dados, incluindo colunas de contagem de nulos e porcentagem de nulos em cada variável\n",
    "\n",
    "  Args:\n",
    "      df (pd.DataFrame): base de dados para descrição\n",
    "      percentiles (bool, optional): exibição dos percentis [0.05, 0.25, 0.5, 0.75, 0.9, 0.99]. Defaults to False.\n",
    "\n",
    "  Returns:\n",
    "      des(pd.DataFrame): descrição dos dados com a quantidade e porcentagem de nulos por feature\n",
    "  \"\"\"  \n",
    "  import pandas as pd\n",
    "\n",
    "  if percentiles is True:\n",
    "    des = df.describe(datetime_is_numeric=True,\n",
    "                      percentiles=[0.05, 0.25, 0.5, 0.75, 0.9, 0.99]).T\n",
    "    des[\"nullCount\"] = df.shape[0] - des[\"count\"]\n",
    "    des[\"null%\"] = (des[\"nullCount\"] / df.shape[0]) * 100\n",
    "\n",
    "  else:\n",
    "    des = df.describe(include='all', \n",
    "                      datetime_is_numeric=True).T\n",
    "    des['nullCount'] = df.shape[0] - des['count']\n",
    "    des['null%'] = (des['nullCount'] / df.shape[0]) * 100\n",
    "    \n",
    "  return des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_filter(df, columns, entries, drop_first=True):\n",
    "  \"\"\"Filtro de categorias, gera uma nova categoria Others com os menos representativos, retornando nova base com os dados dummyficados\n",
    "\n",
    "  Args:\n",
    "      df (pd.DataFrame): base de dados\n",
    "      columns (list): lista com as colunas com dados categóricos\n",
    "      entries (int): quantidade mínima de entradas significativas de categorias\n",
    "      drop_first(bool, optional): remoção da primeira categoria para evitar multicolinearidade. Defaults to True.\n",
    "\n",
    "  Returns:\n",
    "      df(pd.DataFrame): base de dados com a dummyficação dos valores categóricos\n",
    "  \"\"\"  \n",
    "  import pandas as pd\n",
    "  \n",
    "  filter = {}\n",
    "  for col in columns:\n",
    "    cat = df[col].value_counts().sort_values(ascending=True)\n",
    "    filter = {x: 'Other' for x in cat[cat < entries].index}\n",
    "    \n",
    "  df.loc[:, col] = df[col].replace(filter).copy()\n",
    "  df = pd.get_dummies(df, columns=columns, drop_first=drop_first)\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histplot(data, title=None, xlabel=None, ylabel=None, x=None, y=None, ax=False, nrows=1, ncols=1, figsize=(10, 5), bins=10):\n",
    "  \"\"\"Gráfico de frequência pra visualização de dados numéricos e categóricos\n",
    "\n",
    "  Args:\n",
    "      data (pd.DataFrame): base de dados\n",
    "      title (str, optional): título do gráfico, opcional. Defaults to None.\n",
    "      xlabel (str, optional): título do eixo x, opcional. Defaults to None.\n",
    "      ylabel (str, optional): título do eixo y, opcional. Defaults to None.\n",
    "      x (str, optional): feature da base de dados para o eixo x. Defaults to None.\n",
    "      y (str, optional): feature da base de dados para o eixo y. Defaults to None.\n",
    "      ax (bool, optional): determina se a impressão conterá múltiplos gráficos. Defaults to False.\n",
    "      nrows (int, optional): quantidade de linhas. Defaults to 1.\n",
    "      ncols (int, optional): quantidade de colunas. Defaults to 1.\n",
    "      figsize (tuple, optional): tamanho da impressão. Defaults to (10, 5).\n",
    "      bins (int, optional): quantidade de bins. Defaults to 10.\n",
    "\n",
    "  Returns:\n",
    "      sns.histplot: impressão do gráfico\n",
    "  \"\"\"  \n",
    "  import matplotlib.pyplot as plt\n",
    "  import seaborn as sns\n",
    "  \n",
    "  if ax is True:\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    c = 0\n",
    "    if len(x) < (nrows * ncols):      \n",
    "      for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "          if i == nrows-1 and j == ncols-1:\n",
    "            fig.delaxes(axes[nrows-1,ncols-1])\n",
    "          else:\n",
    "            sns.histplot(data=data, x=x[c], y=y, bins=bins, ax=axes[i,j]).set_ylabel(ylabel)\n",
    "            c += 1\n",
    "      \n",
    "    elif nrows > 1 and ncols > 1:      \n",
    "      for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "          sns.histplot(data=data, x=x[c], y=y, bins=bins, ax=axes[i,j]).set_ylabel(ylabel)\n",
    "          c += 1\n",
    "    elif nrows == 1:\n",
    "      for j in range(ncols):\n",
    "        sns.histplot(data=data, x=x[c], y=y, bins=bins, ax=axes[j]).set_ylabel(ylabel)\n",
    "        c += 1\n",
    "    elif ncols == 1:\n",
    "      for i in range(nrows):\n",
    "        sns.histplot(data=data, x=x[c], y=y, bins=bins, ax=axes[i]).set_ylabel(ylabel)\n",
    "        c += 1\n",
    "    if title is not None:\n",
    "      fig.suptitle(title)\n",
    "  \n",
    "  else:\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.histplot(data=data, x=x, y=y, bins=bins)\n",
    "    \n",
    "    if title is not None:\n",
    "      plt.title(title)\n",
    "    if xlabel is not None:\n",
    "      plt.xlabel(xlabel)\n",
    "    if ylabel is not None:\n",
    "      plt.ylabel(ylabel)\n",
    "      \n",
    "  return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrplot(data):\n",
    "    \"\"\"Gráfico de correlação com paleta de cores do azul ao vermelho e anotações para melhor entendimento dos dados\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): base de dados para realizar a correlação\n",
    "\n",
    "    Returns:\n",
    "        sns.heatmap: impressão do gráfico\n",
    "    \"\"\"    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    sns.heatmap(\n",
    "        data=data.corr(),\n",
    "        cmap=sns.diverging_palette(230, 20, as_cmap=True),\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        annot=True,\n",
    "    )\n",
    "\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_test(df):\n",
    "    \"\"\"Realiza teste de normalidade de Kolmorogov no resíduo da previsão\n",
    "\n",
    "    Args:\n",
    "        df (pd.Series): resíduo da previsão\n",
    "\n",
    "    Returns:\n",
    "        data(pd.DataFrame): tabela com as informações estatísticas e P-Value do resíduo de previsão\n",
    "    \"\"\" \n",
    "    import scipy as sp\n",
    "    import pandas as pd\n",
    "\n",
    "    kStat, kPvalue = sp.stats.kstest(df, 'norm')\n",
    "    data = pd.DataFrame({'kStat': kStat,\n",
    "                         'P-Value': kPvalue},\n",
    "                        index=['Kolmogrov']).round(decimals=5)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_scale(x):\n",
    "    \"\"\"Escalonar as variáveis numéricas da base de dados\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame): base de dados com as variáveis numéricas\n",
    "\n",
    "    Returns:\n",
    "        xnorm(float): valores escalonados\n",
    "    \"\"\"    \n",
    "    import numpy as np\n",
    "\n",
    "    # Calcular a média da variável\n",
    "    mean = np.mean(x, axis=0)\n",
    "\n",
    "    # Calcular o desvio padrão amostral da variável\n",
    "    sigma = np.std(x, axis=0, ddof=1)\n",
    "\n",
    "    # Realizar o escalonamento\n",
    "    xnorm = (x - mean) / sigma\n",
    "\n",
    "    return xnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_formula(df, dependent_var, *excluded_rows):\n",
    "    \"\"\"Fórmula para seleção das colunas para gerar a matriz dos dados\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): base de dados\n",
    "        dependent_var (str): variável resposta\n",
    "        * excluded_rows (str): variáveis a serem excluídas\n",
    "\n",
    "    Returns:\n",
    "        dependent_var(list): relação das colunas para criação da matriz\n",
    "    \"\"\"    \n",
    "\n",
    "    # Listar o nome das colunas do dataframe\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # Remover a variável dependente\n",
    "    cols.remove(dependent_var)\n",
    "\n",
    "    # Remover as variáveis excluídas\n",
    "    for col in excluded_rows:\n",
    "        cols.remove(col)\n",
    "\n",
    "    # Retornar a fórmula\n",
    "    return dependent_var + \" ~ \" + \" + \".join(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(y, x, type='OLS', alpha=1.0):\n",
    "  \n",
    "  \"\"\"Ajuste do modelo para os métodos OLS e Ridge\n",
    "\n",
    "  Args:\n",
    "      y (pd.DataFrame): variável resposta\n",
    "      x (pd.DataFrame): regressores\n",
    "      type (str, optional): tipo de modelo OLS ou Ridge. Defaults to 'OLS'.\n",
    "      alpha (float, optional): alpha do modelo Ridge para ajuste. Defaults to 1.0.\n",
    "\n",
    "  Returns:\n",
    "      regfit(): modelo ajustado\n",
    "  \"\"\"  \n",
    "  import statsmodels.api as sm\n",
    "  from sklearn.linear_model import Ridge\n",
    "  \n",
    "  if type == 'OLS':\n",
    "    reg = sm.OLS(y, x)\n",
    "    regfit = reg.fit()\n",
    "  elif type == 'Ridge':\n",
    "    regfit = Ridge(alpha=alpha)\n",
    "    regfit.fit(x, y)\n",
    "\n",
    "  return regfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif_view(x, intercept=False):\n",
    "    \"\"\"Fator de inflação da variância\n",
    "\n",
    "    Args:\n",
    "            x (pd.DataFrame): base de dados dos regressores\n",
    "            intercept (bool, optional): considerar ou não a coluna Intercept. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "            vif(pd.DataFrame): tabela com as variáveis e seus respectivos VIFs\n",
    "    \"\"\"    \n",
    "    import pandas as pd\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "    if intercept is True:\n",
    "            vif = pd.DataFrame({'Variáveis': x.columns[1:],\n",
    "                                'VIF': [variance_inflation_factor(x.values, i + 1)\n",
    "                                        for i in range(len(x.columns[1:]))]\n",
    "                            })\n",
    "    else:\n",
    "            vif = pd.DataFrame({'Variáveis': x.columns,\n",
    "                                'VIF': [variance_inflation_factor(x.values, i + 1)\n",
    "                                        for i in range(len(x.columns))]\n",
    "                            })\n",
    "            \n",
    "    return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_table(y, fit, columns, residual=False):\n",
    "    \"\"\"Tabela de previsão e cálculo de resíduo do modelo\n",
    "\n",
    "    Args:\n",
    "        y (pd.DataFrame): variável resposta\n",
    "        fit (.predict): predição do modelo ajustado dos regressores\n",
    "        columns (list): nome das colunas Real e Previsto\n",
    "        residual (bool, optional): cálculo do residual dos regressores. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pred(pd.DataFrame): base com predição do modelo ajustado\n",
    "    \"\"\"    \n",
    "    import pandas as pd\n",
    "    \n",
    "    pred = pd.concat([y, fit], axis=1)\n",
    "    pred.columns = columns\n",
    "\n",
    "    if residual is True:\n",
    "        pred['Res'] = pred[columns[0]] - pred[columns[1]]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_quality(pred):\n",
    "    \"\"\"Qualidade de ajuste do modelo com R² e RMSE\n",
    "\n",
    "    Args:\n",
    "        pred (pd.DataFrame): tabela de previsão do modelo\n",
    "\n",
    "    Returns:\n",
    "        adj(pd.DataFrame): cálculo de R² e RMSE do modelo\n",
    "    \"\"\"    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "\n",
    "    adj = pd.DataFrame({\n",
    "        'R²': metrics.r2_score(pred.loc[:, pred.columns[0]], \n",
    "                               pred.loc[:, pred.columns[1]]),\n",
    "        'RMSE': np.sqrt(metrics.mean_squared_error(pred.loc[:, pred.columns[0]], \n",
    "                                                   pred.loc[:, pred.columns[1]]))\n",
    "    }, index=[0])\n",
    "\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcao_perda(x, y, beta):\n",
    "    \"\"\"Cria a função de perda para o gradiente descendente\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame): matriz dos regressores\n",
    "        y (pd.Series): série com a resposta\n",
    "        beta (np.array): série dos parâmetros\n",
    "\n",
    "    Returns:\n",
    "        perda: cálculo da perda\n",
    "    \"\"\"    \n",
    "    import numpy as np\n",
    "\n",
    "    m, n = x.shape\n",
    "\n",
    "    # Previsão da resposta: produto escalar entre os vetores de parâmetros e o dataframe com os regressores - produto escalar entre x e beta\n",
    "    pred = x.dot(beta)\n",
    "\n",
    "    # Calcular os erros de previsão: é a diferença entre o valor real da variável y e o valor previsto pelo modelo - resíduo\n",
    "    res = np.subtract(pred, y.squeeze())\n",
    "\n",
    "    # Calcular o quadrado dos erros\n",
    "    sqrRes = np.square(res)\n",
    "\n",
    "    # Calcular a perda\n",
    "    perda = 1 / (2 * m) * np.sum(sqrRes)\n",
    "\n",
    "    return perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_des(x, y, alpha, itera):\n",
    "    \"\"\"Cálculo do gradiente descendente\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame): matriz dos regressores\n",
    "        y (pd.Series): série da resposta\n",
    "        alpha (float): taxa de aprendizagem escalar do modelo\n",
    "        itera (int): número de iterações do algoritmo\n",
    "\n",
    "    Returns:\n",
    "        beta(pd.Series): série com os valores finais dos parâmetros\n",
    "        hPerda(pd.Series): série com o histórico de perdas\n",
    "    \"\"\"    \n",
    "    import numpy as np\n",
    "\n",
    "    # Define o número de linhas e o de regressores\n",
    "    m, n = x.shape\n",
    "    \n",
    "    # Inicializa os valores dos betas\n",
    "    beta = np.zeros(n)\n",
    "\n",
    "    # Inicializa a série do histórico de perdas: ajuda a encontrar os melhores valores de alpha para alcançar uma melhor convergência\n",
    "    hPerda = np.zeros(itera)\n",
    "\n",
    "    for i in range(itera):\n",
    "\n",
    "        # Calcula as previsões com os valores atuais dos parâmetros\n",
    "        pred = x.dot(beta)\n",
    "\n",
    "        # Calcula os resíduos de previsão: previsão - resposta\n",
    "        res = np.subtract(pred, y.squeeze())\n",
    "\n",
    "        # Calcula o incremento/decremento valor dos betas - derivada parcial da função perda:\n",
    "        # quanto maior o alpha, maior o passo que o algoritmo dará para o próximo beta\n",
    "        sumDelta = (alpha / m) * x.transpose().dot(res)\n",
    "\n",
    "        # Atualizar os valores dos betas do modelo\n",
    "        beta = beta - sumDelta\n",
    "\n",
    "        # Calcula a nova perda com os novos valores dos betas\n",
    "        hPerda[i] = funcao_perda(x, y, beta)\n",
    "\n",
    "    return beta, hPerda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Carregar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Análise Exproratória dos dados\n",
    "\n",
    "## 3.1. Separação das variáveis e tratamento de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_null(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Variáveis categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = []\n",
    "\n",
    "dfCat = df.loc[:, c]\n",
    "dfCat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Variáveis numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = []\n",
    "resp = \n",
    "# num = list(df.select_dtypes(include=''))\n",
    "# num.remove(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNum = df.loc[:, num]\n",
    "dfNum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_null(dfNum, percentiles=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResp = df.loc[:, resp]\n",
    "dfResp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Distribuição das variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(data=dfCat, \n",
    "         title='Distribuição das variáveis categóricas', \n",
    "         ylabel='Frequência',\n",
    "         x=cat, ax=True,\n",
    "         ncols=2, nrows=,\n",
    "         bins=15,\n",
    "         figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(data=dfNum, \n",
    "         title='Distribuição das variáveis numéricas', \n",
    "         ylabel='Frequência',\n",
    "         x=num, ax=True,\n",
    "         ncols=2, nrows=,\n",
    "         bins=15,\n",
    "         figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(data=dfResp, x=resp,\n",
    "         ylabel='Frequência', xlabel='',\n",
    "         title='Distribuição ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pré Tratamento de dados\n",
    "## 4.1. Resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test(dfResp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Transformação logarítmica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(data=np.log(dfResp), title='ln do ',\n",
    "         ylabel='Frequência', xlabel='ln ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test(np.log(dfResp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Transformação Box-Cox\n",
    "Encontrar um exponente que leva a um desvio padrão menor da resposta - achata o gráfico para que ele se pareça mais com a distribuição normal.\n",
    "\n",
    "Apha representa o intervalo de confiança"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxcox = sp.stats.boxcox(dfResp, alpha=0.01)\n",
    "print(f'Melhor valor da transformação: {boxcox[1]: .3f} \\n'\n",
    "      f'Intervalo de confiança 99%: {boxcox[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2.1. Lambda Ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(data=boxcox[0], title=' - Box Cox - Lambda ótimo',\n",
    "         ylabel='Frequência', xlabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test(boxcox[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolher o método que melhor performar no teste de normalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Variáveis numéricas\n",
    "### 4.2.1. Escalonamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xNorm = feat_scale(dfNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Avaliação de multicolinearidade e correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(xNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corrplot(xNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Variáveis categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xCat = cat_filter(dfCat.loc[:, cat], cat, algumnum)\n",
    "\n",
    "xCat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ajuste do modelo e teste de significância"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Dividir o dataset em Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([xNorm, xCat, dfResp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = pt.dmatrices(ols_formula(df_final, ''),\n",
    "                     data=df_final, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_Train, x_Test, y_Train, y_Test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Método Mínimos Quadrados\n",
    "### 5.2.1. Ajuste do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regfitOLS_original = fit_model(y_Train, x_Train)\n",
    "print(regfitOLS.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_view(x_Train, intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. Qualidade de ajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predY = predict_table(y= y_Test,\n",
    "                     fit= regfitOLS_original.predict(x_Test),\n",
    "                     columns= ['Real', 'Previsto'],\n",
    "                     residual=True)\n",
    "predY.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_quality(predY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3. Análise de resíduos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(data=predY,\n",
    "         x='Res', xlabel='Resíduos para ',\n",
    "         ylabel='Frequência',\n",
    "         title='Distribuição dos resíduos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test(predY['Res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.regplot(data=predY,\n",
    "            x='Previsto',\n",
    "            y='Res')\n",
    "\n",
    "plt.title('Avaliação da homoscedasticidade da previsão')\n",
    "plt.xlabel(' Previsto')\n",
    "plt.ylabel('Resíduo')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Método Gradiente Descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantidade de iterações\n",
    "itera = 1000\n",
    "\n",
    "# Taxa de aprendizado\n",
    "alpha = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se for escolhida a trsnformação ln, converter y_Train para np.log()\n",
    "beta, hPerda = grad_des(x=x_Train,\n",
    "                        y=y_Train,\n",
    "                        alpha=alpha, itera=itera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Convergência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.lineplot(x=range(1, itera + 1), y=hPerda)\n",
    "\n",
    "plt.xlabel(\"Iterações\")\n",
    "plt.ylabel(\"Erro quadrado médio\")\n",
    "plt.title(\"Histórico de perdas\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. Qualidade de ajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se for escolhida a trsnformação ln, converter fit para np.exp()\n",
    "predGradY = predict_table(y=y_Test,\n",
    "                          fit=np.exp(x_Test.dot(beta)),\n",
    "                          columns= ['Real', 'Previsto'],\n",
    "                          residual=True)\n",
    "predGradY.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_quality(predGradY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Análise de resíduos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(data=predGradY,\n",
    "         x='Res', xlabel='Resíduos para ',\n",
    "         ylabel='Frequência',\n",
    "         title='Distribuição dos resíduos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test(predGradY['Res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.regplot(data=predGradY,\n",
    "            x='Previsto',\n",
    "            y='Res')\n",
    "\n",
    "plt.title('Avaliação da homoscedasticidade da previsão Gradiente Descendente')\n",
    "plt.xlabel(' Previsto')\n",
    "plt.ylabel('Resíduo')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Método Regressão Regularizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1. Definição do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regRidge = Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2. Procura do melhor alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid de pesquisa\n",
    "grid = dict()\n",
    "grid['alpha'] = np.arange(0, 1, 0.01)\n",
    "\n",
    "# Método de pesquisa kFold\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Definir a pesquisa\n",
    "search = GridSearchCV(\n",
    "    regRidge, grid, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar a pesquisa\n",
    "results = search.fit(x_Train, np.log(y_Train))\n",
    "alpha = results.best_params_['alpha']\n",
    "\n",
    "# Retornar o melhor valor\n",
    "print(f'Melhor alpha: {alpha:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3. Ajuste do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regRidge2 = fit_model(x=x_Train, y=y_Train,\n",
    "                      type='Ridge', alpha=alpha)\n",
    "\n",
    "(pd.DataFrame(regRidge2.coef_, columns=list(x_Train), index=['coef'])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.4. Qualidade de ajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predRidge = predict_table(y=y_Test,\n",
    "                          fit=pd.DataFrame(regRidge2.predict(x_Test)),\n",
    "                          columns= ['Real', 'Previsto'],\n",
    "                          residual=True)\n",
    "predRidge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_quality(predRidge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.5. Análise de resíduos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(data=predRidge,\n",
    "         x='Res', xlabel='Resíduos para ',\n",
    "         ylabel='Frequência',\n",
    "         title='Distribuição dos resíduos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test(predRidge['Res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.regplot(data=predRidge,\n",
    "            x='Previsto',\n",
    "            y='Res')\n",
    "\n",
    "plt.title('Avaliação da homoscedasticidade da previsão Regressão Regularizada')\n",
    "plt.xlabel(' Previsto')\n",
    "plt.ylabel('Resíduo')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
